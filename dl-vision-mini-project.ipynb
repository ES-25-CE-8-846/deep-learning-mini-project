{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3173719,"sourceType":"datasetVersion","datasetId":952827}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport wandb\nimport random\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_api = user_secrets.get_secret(\"wandb-api\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Login to wandb\nwandb.login(key=wandb_api)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1. Explore the dataset through code\n\n### a) How many images does the dataset contain?","metadata":{}},{"cell_type":"code","source":"# Count the images in the test-, train-, and validation folders.\ndata_folder = Path(\"/kaggle/input/fruit-and-vegetable-image-recognition/\")\ntest_folder = data_folder / \"test\"\ntrain_folder = data_folder / \"train\"\nvalidation_folder = data_folder / \"validation\"\n\nprint(f'Total files in dataset: {sum(1 for file in data_folder.rglob(\"*\") if file.is_file())} folders')\nprint(f'Files in test folder: {sum(1 for file in test_folder.rglob(\"*\") if file.is_file())} folders')\nprint(f'Files in train folder: {sum(1 for file in train_folder.rglob(\"*\") if file.is_file())} folders')\nprint(f'Files in validation folder: {sum(1 for file in validation_folder.rglob(\"*\") if file.is_file())} folders')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### b) How many classes? How many images per class? Show a histogram of the number of instances per class.","metadata":{}},{"cell_type":"code","source":"def plot_histogram(class_count_dictionary):\n    classes = list(class_count_dictionary.keys())\n    counts = list(class_count_dictionary.values())\n    \n    # Create the bar chart\n    plt.figure(figsize=(12, 6))  # Set figure size\n    plt.bar(classes, counts, color='skyblue')\n    \n    # Labeling\n    plt.xlabel('Class', fontsize=12)\n    plt.ylabel('Number of Images', fontsize=12)\n    plt.title('Number of Images per Class', fontsize=14)\n    plt.xticks(rotation=90)  # Rotate class names for readability\n\n    # Show the plot\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print number of classes in the dataset\nn_classes_test = [x for x in test_folder.glob(\"*\") if not x.is_file()]\nn_classes_train = [x for x in train_folder.glob(\"*\") if not x.is_file()]\nn_classes_validation = [x for x in validation_folder.glob(\"*\") if not x.is_file()]\n\nprint(f'Amount of classes: {len(n_classes_test)}')\n\n# Compute avg number of images per class\nclass_count_test = {}\nclass_count_train = {}\nclass_count_validation = {}\n\n# Count images per class in the test split\nfor class_ in n_classes_test:\n    counter = 0\n    for file in class_.glob(\"*\"):\n        if file.is_file():\n            counter += 1\n    class_count_test[class_.name] = counter\n\n# Count images per class in the train split\nfor class_ in n_classes_train:\n    counter = 0\n    for file in class_.glob(\"*\"):\n        if file.is_file():\n            counter += 1\n    class_count_train[class_.name] = counter\n\n# Count images per class in the validation split\nfor class_ in n_classes_validation:\n    counter = 0\n    for file in class_.glob(\"*\"):\n        if file.is_file():\n            counter += 1\n    class_count_validation[class_.name] = counter\n\nprint(f'Average images per class in test-split: {sum(class_count_test.values()) / len(class_count_test)}')\nprint(f'Average images per class in train-split: {sum(class_count_train.values()) / len(class_count_train)}')\nprint(f'Average images per class in validation-split: {sum(class_count_validation.values()) / len(class_count_validation)}')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot histogram for test-split:\nplot_histogram(class_count_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot histogram for train-split:\nplot_histogram(class_count_train)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot histogram for validation-split:\nplot_histogram(class_count_validation)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### c) Show 4 random images from each class","metadata":{}},{"cell_type":"code","source":"import random\nimport matplotlib.pyplot as plt\nimport cv2\n\ntest_images = Path(\"/kaggle/input/fruit-and-vegetable-image-recognition/test\")\n\nfor class_folder in test_images.iterdir():\n    image_files = list(class_folder.glob(\"*.*\"))\n\n    selected_images = random.sample(image_files, 4)  # Select 4 random images\n        \n    # Display images\n    fig, axes = plt.subplots(1, len(selected_images), figsize=(12, 4))\n    for img_path, ax in zip(selected_images, axes):\n        img = cv2.imread(str(img_path))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        ax.imshow(img)\n        ax.axis(\"off\")\n        \n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### d) Describe if/how you think the data distribution will affect training of a classifier.\n\nA model might overfit to classes with higher representation in the training data. This means that we may get more false positives for those classes and fewer true positives for the rest of the classes. To mitigate this, data balancing can be done with eg. augmented data or training on a subset of the available data though this may introduce other problems.","metadata":{}},{"cell_type":"markdown","source":"### e) Decide what part of the dataset to use; all, some classes, some samples. Motivate your choice. \n\nAs stated in d) we could benefit from choosing a subset of the available data. However, it is important to have representation from all classes that we want to be able to classify with our model. More data is also preferable as the model will be more generalized.\n","metadata":{}},{"cell_type":"markdown","source":"# 2. Use a neural network of your own choice to classify the dataset. Explain your choice and at least one alternative. Document your experiences:\n","metadata":{}},{"cell_type":"code","source":"\nimport torch \nimport torchvision \nfrom torchinfo import torchinfo\nimport torch.utils.data as data\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport os\nimport cv2 \nfrom PIL import Image \nfrom tqdm import tqdm\n\nclass FruitsAndVeggies(Dataset):\n    def __init__(self, split_root, transforms):\n        # create a dict of labels and filepaths\n        class_dir_names = sorted(os.listdir(split_root))\n        n_classes = len(class_dir_names)\n        self.transforms = transforms\n        # create one-hot encoding \n        self.dataset_list = []\n        for i, class_dir in enumerate(class_dir_names):\n            label = torch.zeros(n_classes)\n            label[i] = 1\n            \n            extension_set = {\"jpg\", \"png\",  \"JPG\", \"jpeg\"}\n\n            for image in sorted(os.listdir(os.path.join(split_root, class_dir))):\n                extension = image.split(\".\")[-1]\n\n                if extension in extension_set:\n                    self.dataset_list.append([label, os.path.join(split_root,class_dir,image)])\n                else:\n                    print(f\"{extension} found in dataset\")\n        \n    def __len__(self):\n        return len(self.dataset_list)\n\n    def __getitem__(self, index):\n        data_list = self.dataset_list[index]\n        image_path = data_list[1]\n        #label = data_list[0]\n        label = torch.argmax(data_list[0])  # Converts one-hot to class index\n        image = Image.open(image_path)\n       \n\n        if image.mode != \"RGB\":\n            image = image.convert(\"RGB\")\n\n        image = self.transforms(image)\n\n        return image, label    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nimport os\n\n# Hyperparameters\nepochs = 10\nlearning_rate = 1e-4\nbatch_size = 32\n\n# Define data transformations\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),  \n    transforms.ToTensor(),\n])\n\n# Load dataset\npath_to_data = \"/kaggle/input/fruit-and-vegetable-image-recognition/\"\ntrain_dataset = FruitsAndVeggies(os.path.join(path_to_data, \"train\"), transform)\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n\nval_dataset = FruitsAndVeggies(os.path.join(path_to_data, \"validation\"), transform)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n\ntest_dataset = FruitsAndVeggies(os.path.join(path_to_data, \"test\"), transform)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n\n# Get number of classes\nnum_classes = len(os.listdir(os.path.join(path_to_data, \"train\")))\n\n\nclass CustomCNN(nn.Module):\n    def __init__(self, num_classes, num_conv_layers=3, num_filters=32, kernel_size=3, fc_neurons=256):\n        super(CustomCNN, self).__init__()\n\n        layers = []\n        in_channels = 3\n        out_channels = num_filters\n\n        for i in range(num_conv_layers):\n            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=1, padding=1))\n            layers.append(nn.ReLU())\n            layers.append(nn.MaxPool2d(2, 2))\n            in_channels = out_channels\n            out_channels *= 2  # Increase filters for deeper layers\n\n        self.conv_layers = nn.Sequential(*layers)\n\n        # Compute flattened feature size dynamically (assuming input is 224x224)\n        with torch.no_grad():\n            sample_input = torch.zeros(1, 3, 224, 224)\n            sample_output = self.conv_layers(sample_input)\n            flattened_size = sample_output.view(1, -1).shape[1]\n\n        self.fc_layers = nn.Sequential(\n            nn.Linear(flattened_size, fc_neurons),\n            nn.ReLU(),\n            nn.Linear(fc_neurons, num_classes)\n        )\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = x.view(x.size(0), -1)  # Flatten before fully connected layers\n        x = self.fc_layers(x)\n        return x","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sweep_config = {\n    \"method\": \"grid\",\n    \"metric\": {\"name\": \"val_accuracy\", \"goal\": \"maximize\"},\n    \"parameters\": {\n        \"learning_rate\": {\"values\": [1e-3, 1e-4, 1e-5]},\n        \"batch_size\": {\"values\": [16, 32, 64]},\n        \"num_conv_layers\": {\"values\": [2, 3, 4]},\n        \"epochs\": {\"values\": [10, 15, 20]},\n    }\n}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sweep_id = wandb.sweep(sweep_config, project=\"Mini-project\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train():\n    # Initialize wandb for this specific run\n    run = wandb.init(entity=\"avs-846\")\n\n    # Load hyperparameters from wandb.config\n    learning_rate = wandb.config.learning_rate\n    batch_size = wandb.config.batch_size\n    num_conv_layers = wandb.config.num_conv_layers\n\n    # Update data loaders with new batch size\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n\n    # Initialize the model with dynamic parameters\n    model = CustomCNN(num_classes=num_classes, num_conv_layers=num_conv_layers)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    # Training loop with tqdm\n    for epoch in range(epochs):\n        model.train()\n        train_loss, train_correct, total_train = 0, 0, 0\n        train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} Training\", leave=False)\n\n        for images, labels in train_bar:\n            images, labels = images.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n            train_correct += (outputs.argmax(1) == labels).sum().item()\n            total_train += labels.size(0)\n\n            train_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n\n        train_accuracy = 100 * train_correct / total_train\n        avg_train_loss = train_loss / len(train_loader)\n\n        # Validation loop with tqdm\n        model.eval()\n        val_loss, val_correct, total_val = 0, 0, 0\n        val_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{epochs} Validation\", leave=False)\n\n        with torch.no_grad():\n            for images, labels in val_bar:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs, labels)\n\n                val_loss += loss.item()\n                val_correct += (outputs.argmax(1) == labels).sum().item()\n                total_val += labels.size(0)\n\n                val_bar.set_postfix(loss=f\"{loss.item():.4f}\")\n\n        val_accuracy = 100 * val_correct / total_val\n        avg_val_loss = val_loss / len(val_loader)\n\n        print(f\"Epoch [{epoch+1}/{epochs}] - \"\n              f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_accuracy:.2f}% - \"\n              f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n\n        # Log metrics to wandb\n        wandb.log({\n            \"train_loss\": avg_train_loss,\n            \"train_accuracy\": train_accuracy,\n            \"val_loss\": avg_val_loss,\n            \"val_accuracy\": val_accuracy\n        })\n\n    # Final Test Accuracy\n    model.eval()\n    test_correct, total_test = 0, 0\n\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n\n            test_correct += (outputs.argmax(1) == labels).sum().item()\n            total_test += labels.size(0)\n\n    test_accuracy = 100 * test_correct / total_test\n    print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n    wandb.log({\"test_accuracy\": test_accuracy})\n\n    wandb.finish()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wandb.agent(sweep_id, function=train, count=81)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n### a) Discuss at least four relevant hyper-parameters","metadata":{}},{"cell_type":"markdown","source":"Learning rate:\nThe learning rate is important as it determines how rigid the model weights are during training. With a low learning rate, the weights will change very slowly which can result in long training time. It can also result in the model being \"caught\" in a local minimum. On the other hand, if the learning rate is too high, it can result in the model not being able to converge at a minimum, as it \"overshoots\" at every step.\n\nLearning decay:\nThe learning decay is a hyperparameter that can help to mitigate some of the problems with the learning rate. Though it has not been used in this case, it could be implemented to control the training step even more. The learning decay makes it possible to decrease the learning rate during training, giving the model the opportunity to quickly approach a good result and later on slowly increment towards a minimum loss to avoid overshooting.\n\nEpochs:\nThe number of epochs determines how long the model should be training for. If the model does not train long enough, it may still be able to improve, and with too many epochs, the model could start overfitting. A good way of avoiding this is to use validation data to validate after each epoch. If at some point the validation loss starts increasing while the training loss is still decreasing, it is a good indication that the model is being overfitted to the training data.\n\nBatch size:\nThe batch size determines how much of the data we are working with at once. This means that the weights are only updated after each batch. Larger batch sizes will likely positively impact the generalization capabilities of the model, as weights will be updated to fit several different inputs at the same time and not hyperfixate on a single input at every iteration. However, as the entire batch is loaded into memory at the same time, it is quite memory intensive. Generalization can also be too much, and the model might become biased and unable to handle edge cases.","metadata":{}},{"cell_type":"markdown","source":"### b) Experiment with the effect of different batch sizes","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### c) Experiment with the effect of different learning rates","metadata":{}},{"cell_type":"markdown","source":"### d) Experiment with different number of network layers\n\n### e) Implement at least two data augmentation techniques. Motivate your choices.\n\n\n### f) Discuss what influences the memory use of a solution such as yours. What can be done to reduce this?","metadata":{}},{"cell_type":"markdown","source":"### g) Make sure to distribute the experiments between group members. Include a plot in the report that shows user id vs. e.g. performance metric. To do this, make sure that a user id is logged as one of the parameters to wandb.","metadata":{}},{"cell_type":"markdown","source":"# 3. Break down the results of your classifier. Some suggestions:\n\n### a) Which classes are easy and which are hard? Discuss why.","metadata":{}},{"cell_type":"markdown","source":"### b) Show five examples where the classification went well, and five where the classification failed. What could be some reasons that classification failed and what can be done to improve?","metadata":{}}]}